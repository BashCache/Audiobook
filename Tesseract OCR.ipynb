{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import re\n",
    "import imutils\n",
    "from pytesseract import Output\n",
    "import cv2\n",
    "import os\n",
    "from gtts import gTTS\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'handwritten'\n",
    "os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF as input\n",
    "Convert PDF to image first and then apply pre-processing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "pages = convert_from_path('2018103592 partb q1-tools.pdf', poppler_path=r'C:\\Users\\admin\\Downloads\\poppler-0.68.0_x86\\poppler-0.68.0\\bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "# print(pages)\n",
    "for pg in pages:\n",
    "    filename = dir_name + str(i) + \".jpg\"\n",
    "    i = i+1\n",
    "    completeName = os.path.join(dir_name, filename)\n",
    "#     print(completeName)\n",
    "    pg.save(completeName, 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE- PROCESSING\n",
    "For now only the last page is pre-processed and worked upon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to GrayScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeName = '15.png'\n",
    "img = cv2.imread(completeName)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting the orientation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_num': 0, 'orientation': 0, 'rotate': 0, 'orientation_conf': 0.93, 'script': 'Latin', 'script_conf': 3.62} 0 <class 'dict'> 0\n"
     ]
    }
   ],
   "source": [
    "newdata=pytesseract.image_to_osd(img, output_type=Output.DICT)\n",
    "print(newdata, newdata['rotate'], type(newdata), newdata['orientation'])\n",
    "img = imutils.rotate_bound(img, newdata['rotate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow('Image',img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['level', 'page_num', 'block_num', 'par_num', 'line_num', 'word_num', 'left', 'top', 'width', 'height', 'conf', 'text'])\n",
      "681\n",
      "There are several classic spatial filters for reducing or elimin\n",
      "from images. The mean filter, the median filter and the closing o\n",
      "used. ‘The mean filter is a lowpass or smoothing filter that rep|\n",
      "the neighborhood mean. It reduces the image noise but blurs the\n",
      "filter calculates the median of the pixel neighborhood for each F\n",
      "blurring effect. Finally, the opening closing filter is a mathem\n",
      "that combines the same number of erosion and dilation morpho\n",
      "to eliminate small objects from images.\n",
      "\n",
      "‘The main goal was to train a neural network in a supervised\n",
      "image from a noisy one. In this particular case, it was much ea\n",
      "noisy image from a clean one than to clean a subset of noisy\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "text = pytesseract.image_to_string(img, lang = 'eng')\n",
    "\n",
    "d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "print(d.keys())\n",
    "\n",
    "n_boxes = len(d['text'])\n",
    "for i in range(n_boxes):\n",
    "    if int(d['conf'][i]) > 60:\n",
    "        (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "        img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(len(text))\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proc. Int. Conf. on Information Technology and Business Intelligence (2009) 117-125  RIE SISE) exo nom  a) (b)  Fig. 5. Some of the misclassified word images                                                              (a) Recognition error in the 3\" character (b) Internal segmentation in the 8\" character  As shown in Table 2(a-e), the overall character-level recognition accuracy of the developed system is around 87.98%. The overall character misclassification rate is observed as around 9.73%. Segmentation failures in the document pages account for around 2.29% error cases. The reason behind high segmentation failure is due to the over-segmentation of some of the constituent characters like ‘i’, 'j' and also due to under-segmentation of cursive words in the document pages. The designed system rejects around 9.24% characters in the test dataset. This is mainly due to the presence of multi-skewed handwritten text lines in the test documents. Completely cursive words were also rejected completely in many cases during the experimentation. Some of the sample word images successfully segmented and recognized by Tesseract are shown in Fig. 4. Fig. 5(a-b) shows some of the word images with erroneous segmentation and recognition results.  A major drawback of the current system is its failure to avoid over-segmentation in some of the characters. Also the system fails to segment cursive words in many cases leading to under- segmentation and rejection. The recognition performance of the designed system may further be improved by incorporating more training samples for each user and inclusion of word-level dictionary matching techniques.  Despite these limitations, the designed recognition engine is successfully integrated with the iJIT system for online interpretation of handwritten textual annotations. The word-level recognition time of the OCR engine, as observed on reasonably powered computer hardware, is also found to be satisfactory. In a nutshell, the current work effectively customizes an open source OCR engine for segmentation and recognition of handwritten textual annotations of multiple users within the designed iJ/T system.  6. References  1] www.anoto.com  2] Bertrand Cotiasnon - Jean Camillerapp - lvan Leplumey, “Access by content to handwritten archive documents: generic document recognition method and platform for annotations”, IJDAR (2007) 9: 223— 242.  3] Matthew Ma, Chi Zhang and Patrick Wang, “Studies of Radical Model for Retrieval of Cursive Chinese Handwritten Annotations”, SSPR&SPR 2000, LNCS 1876, pp. 407-416, 2000.  4] Sargur Srihari, Anantharaman Ganesh, Catalin Tomai, Yong-Chul Shin, and Chen Huang, “Information Retrieval System for Handwritten Documents”, DAS 2004, LNCS 3163, pp. 298-309, 2004.  5] S. Basu, K. Konishi, N. Furukawa, H, Ikeda, “A novel scheme for retrieval of handwritten textual annotations for information Just In Time (iJIT)”, proceedings (CD) of IEEE Region 10 Conference (TENCON) -2008.  6] David Doermann, “The Indexing and Retrieval of Document Images: A Survey”, Computer Vision and Image Understanding archive Volume 70 , Issue 3.  7] R.M. Bozinovic and S.N. Srihari, “Off-line Cursive Script Word Recognition’, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 11,pp 68-83, 1989.  8] B.B. Chaudhuri and U. Pal, “A Complete Printed Bangla OCR System”, Pattern Recognition, vol. 31, No. 5. pp. 531-549, 1998.  9] S. Basu, C. Chawdhuri, M. Kundu, M. Nasipuri, D. K. Basu, “A Two-pass Approach to Pattern Classification”, N.R. Pal et.al. (Eds.), ICONIP, LNCS 3316, pp. 781-786.  10] http://code.google.com/p/tesseract-ocr    \f",
      "\n"
     ]
    }
   ],
   "source": [
    "text = text.replace('\\n', ' ')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'en'\n",
    "output = gTTS(text= text, lang=language)\n",
    "output.save('outp.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
